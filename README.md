# Backstory Consistency Verification with Advanced RAG

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JitamB/Backstory-Consistency-NLP/blob/a_rag/advanced_rag_colab.ipynb)

A production-grade RAG pipeline for verifying character backstory consistency against source books. Achieves <1% hallucination rate through hybrid retrieval, Chain-of-Verification, and citation grounding.

---

## ğŸ”„ Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ADVANCED RAG PIPELINE FLOW                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚    BOOKS     â”‚â”€â”€â”€â–¶â”‚  INGESTION   â”‚â”€â”€â”€â–¶â”‚    INDEX     â”‚                  â”‚
â”‚  â”‚  (100k+ words)    â”‚  (One-time)  â”‚    â”‚   (Chunks)   â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                 â”‚                          â”‚
â”‚                                                 â–¼                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚  BACKSTORY   â”‚â”€â”€â”€â–¶â”‚  DECOMPOSE   â”‚â”€â”€â”€â–¶â”‚    CLAIMS    â”‚                  â”‚
â”‚  â”‚  (User Input)â”‚    â”‚   (LLM)      â”‚    â”‚  (Atomic)    â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                 â”‚                          â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚                      â–¼                                          â–¼          â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚               â”‚   RETRIEVE   â”‚â—€â”€â”€â”€ Agentic Loop â”€â”€â”€â”€â”€â”€â–¶â”‚   RE-RANK    â”‚    â”‚
â”‚               â”‚   (Hybrid)   â”‚                         â”‚   (BGE)      â”‚    â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                      â”‚                                          â”‚          â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                 â–¼                          â”‚
â”‚                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                                          â”‚   VERIFY     â”‚                  â”‚
â”‚                                          â”‚   (CoVe)     â”‚                  â”‚
â”‚                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                 â”‚                          â”‚
â”‚                                                 â–¼                          â”‚
â”‚                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚                                          â”‚   VERDICT    â”‚                  â”‚
â”‚                                          â”‚ âœ“/âœ— + Conf.  â”‚                  â”‚
â”‚                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Step-by-Step Pipeline Flow

### Phase 1: Book Ingestion (One-Time Setup)

**Purpose**: Convert raw book text into searchable, semantically-indexed chunks.

```
Book Text â†’ Chapter Detection â†’ Hierarchical Chunking â†’ Entity Extraction â†’ Embedding â†’ Index
```

#### 1.1 Chapter Detection
- **Input**: Raw book text (e.g., "The Count of Monte Cristo.txt")
- **Process**: Regex patterns detect chapter headers (`CHAPTER 1`, `Part II`, etc.)
- **Output**: Chapter boundaries with narrative positions (0.0 = start, 1.0 = end)

#### 1.2 Hierarchical Chunking
Creates two levels of chunks for optimal retrieval:

| Chunk Type | Size | Purpose |
|------------|------|---------|
| **Parent** | 2048 tokens | Rich context for LLM understanding |
| **Child** | 256 tokens | Precise matching for retrieval |

- Each parent has 8-10 children
- Child IDs link back to parent for "Small-to-Big" retrieval

#### 1.3 Entity Extraction (spaCy)
- **Entities**: Characters, locations, organizations
- **Events**: Action-based relationships between entities
- **Output**: Knowledge graph with entity nodes and event edges

#### 1.4 Embedding Generation
- **Model**: `BAAI/bge-large-en-v1.5` (1024 dimensions)
- **BM25 Index**: Parallel sparse index for keyword matching
- **Hybrid Ready**: Both dense and sparse indices for fusion search

---

### Phase 2: Backstory Decomposition

**Purpose**: Break complex backstory into atomic, verifiable claims.

```
"Harry was raised by the Dursleys at 4 Privet Drive and discovered 
he was a wizard on his 11th birthday"

                    â–¼ LLM Decomposition

Claim 1: [RELATIONSHIP] Harry was raised by the Dursleys
Claim 2: [LOCATION] Harry lived at 4 Privet Drive  
Claim 3: [TEMPORAL] Harry discovered he was a wizard at age 11
Claim 4: [TEMPORAL] The discovery happened on his birthday
```

#### Claim Types Generated:
- **TEMPORAL**: Events with time references
- **RELATIONSHIP**: Character connections
- **LOCATION**: Places and settings
- **TRAIT**: Character attributes
- **EVENT**: Actions and happenings

#### Search Query Generation:
Each claim generates optimized search queries for retrieval.

---

### Phase 3: Hybrid Retrieval

**Purpose**: Find relevant evidence using multiple retrieval strategies.

```
Claim â†’ Query Router â†’ [Semantic Search + BM25] â†’ RRF Fusion â†’ Candidates
```

#### 3.1 Query Routing (Confidence-Based)
| Confidence | Pipeline Mode | Latency |
|------------|---------------|---------|
| â‰¥ 0.85 | FAST: Semantic only | ~2s |
| â‰¥ 0.60 | STANDARD: + HyDE + Rerank | ~5s |
| < 0.60 | FULL: + Agentic re-retrieval | ~15s |

#### 3.2 HyDE (Hypothetical Document Embedding)
- LLM generates a "hypothetical ideal passage"
- Embeds this passage instead of raw query
- Improves semantic matching by 15-20%

#### 3.3 Multi-Query Expansion
Generates 3 query variations:
1. Original claim
2. Keyword-focused version
3. Semantic paraphrase

#### 3.4 Reciprocal Rank Fusion (RRF)
Combines semantic and BM25 results:
```
RRF_score = Î£ 1/(k + rank_i)  where k=60
```

#### 3.5 Temporal Decay Weighting
- **ORIGIN_STORY claims**: Boost early book positions
- **CURRENT_STATE claims**: Boost late book positions
- **Decay formula**: `score *= exp(-decay * |position - target|)`

---

### Phase 4: Cross-Encoder Reranking

**Purpose**: Precisely score query-document relevance.

```
Candidates â†’ BGE-Reranker-v2 â†’ Top-5 Evidence
```

#### Reranker Features:
- **Model**: `BAAI/bge-reranker-v2-m3` (8K context)
- **Batch Processing**: Efficient GPU utilization
- **Recency Weighting**: Combines relevance with temporal scores
- **Score Threshold**: Filters low-confidence matches

---

### Phase 5: Chain-of-Verification (CoVe)

**Purpose**: Generate grounded, citation-backed verification.

```
Claim + Evidence â†’ CoVe Prompt â†’ Structured Verification â†’ Citation Validation
```

#### 5.1 Structured Output (Pydantic)
Returns machine-parseable JSON:
```json
{
  "verdict": "CONSISTENT",
  "confidence": 0.92,
  "sub_claims": [
    {
      "claim": "Harry lived with the Dursleys",
      "verdict": "SUPPORT",
      "evidence": "...the Dursleys, who were his only remaining relatives...",
      "citations": ["chunk_42", "chunk_108"]
    }
  ],
  "reasoning": "All claims verified against source text."
}
```

#### 5.2 Self-Consistency Decoding
- Runs verification 3 times with temperature variation
- Majority vote determines final verdict
- Disagreement lowers confidence score

#### 5.3 Citation Validation
- All citations must reference real evidence chunks
- Invalid citations are flagged and removed
- Confidence penalized for hallucinated citations

---

### Phase 6: Agentic Re-Retrieval (Optional)

**Purpose**: Self-improving loop for low-confidence results.

```
If confidence < 0.6:
    â†’ Extract missing information
    â†’ Generate new queries
    â†’ Retrieve additional evidence
    â†’ Re-verify with expanded context
```

Triggers when:
- Overall confidence below threshold
- >30% sub-claims have insufficient evidence
- Multiple missing information items identified

---

## ğŸ—‚ï¸ Project Structure

```
Backstory-Consistency-NLP/
â”œâ”€â”€ main.py                          # Original pipeline entry point
â”œâ”€â”€ requirements.txt                 # Original dependencies
â”œâ”€â”€ requirements_advanced.txt        # Advanced RAG dependencies
â”œâ”€â”€ .env.example                     # API key template
â”‚
â”œâ”€â”€ advanced_rag/                    # ğŸ†• Advanced RAG Module
â”‚   â”œâ”€â”€ __init__.py                  # Module exports
â”‚   â”œâ”€â”€ config.py                    # Centralized configuration
â”‚   â”œâ”€â”€ pipeline.py                  # Unified pipeline orchestrator
â”‚   â”œâ”€â”€ cache_manager.py             # Semantic result caching
â”‚   â”‚
â”‚   â”œâ”€â”€ indexing/                    # Ingestion & Chunking
â”‚   â”‚   â”œâ”€â”€ graph_chunker.py         # Temporal-aware hierarchical chunking
â”‚   â”‚   â”œâ”€â”€ hierarchical_indexer.py  # Parent-child chunk management
â”‚   â”‚   â””â”€â”€ bm25_indexer.py          # Sparse keyword index
â”‚   â”‚
â”‚   â”œâ”€â”€ retrieval/                   # Search & Retrieval
â”‚   â”‚   â”œâ”€â”€ hybrid_retriever.py      # Semantic + BM25 + RRF fusion
â”‚   â”‚   â”œâ”€â”€ query_router.py          # Confidence-based pipeline routing
â”‚   â”‚   â”œâ”€â”€ query_transformer.py     # HyDE + multi-query expansion
â”‚   â”‚   â”œâ”€â”€ graph_retriever.py       # Entity-based graph traversal
â”‚   â”‚   â””â”€â”€ agentic_retriever.py     # Self-improving retrieval loop
â”‚   â”‚
â”‚   â”œâ”€â”€ reranking/                   # Re-ranking
â”‚   â”‚   â””â”€â”€ cross_encoder_reranker.py  # BGE-Reranker with recency
â”‚   â”‚
â”‚   â”œâ”€â”€ generation/                  # Verification & Output
â”‚   â”‚   â”œâ”€â”€ schemas.py               # Pydantic models (15+ schemas)
â”‚   â”‚   â”œâ”€â”€ cove_prompts.py          # Chain-of-Verification templates
â”‚   â”‚   â””â”€â”€ grounded_generator.py    # Citation-grounded generation
â”‚   â”‚
â”‚   â””â”€â”€ evaluation/                  # Metrics
â”‚       â””â”€â”€ ragas_evaluator.py       # RAG Triad metrics
â”‚
â”œâ”€â”€ Dataset/
â”‚   â”œâ”€â”€ Books/                       # Source book files
â”‚   â”œâ”€â”€ train.csv                    # Training data
â”‚   â””â”€â”€ test.csv                     # Test data
â”‚
â””â”€â”€ advanced_rag_colab.ipynb         # ğŸ†• Colab notebook (GPU)
```

---

## ğŸš€ Quick Start

### Option 1: Google Colab (Recommended)

Click the badge above or [open directly](https://colab.research.google.com/github/JitamB/Backstory-Consistency-NLP/blob/a_rag/advanced_rag_colab.ipynb).

1. Enable GPU: Runtime â†’ Change runtime type â†’ T4
2. Run all cells
3. Paste your GROQ API key in Cell 4

### Option 2: Local Installation

```bash
git clone -b a_rag https://github.com/JitamB/Backstory-Consistency-NLP.git
cd Backstory-Consistency-NLP

python -m venv .venv
source .venv/bin/activate

pip install -r requirements_advanced.txt
python -m spacy download en_core_web_sm

cp .env.example .env
```

---

## ğŸ“– Usage

### Basic Usage

```python
from advanced_rag import AdvancedRAGPipeline

# Initialize
pipeline = AdvancedRAGPipeline()

# Ingest books (one-time)
pipeline.ingest_books('./Dataset/Books')

# Verify a backstory
result = pipeline.verify_backstory(
    backstory="Harry Potter was raised by the Dursleys at 4 Privet Drive...",
    character_name="Harry Potter",
    book_title="Harry Potter"
)

print(f"Consistent: {result.is_consistent}")
print(f"Confidence: {result.confidence:.2%}")
print(f"Verdict: {result.verdict}")
```

### Batch Validation

```python
# Run on test dataset
results = pipeline.run_validation('./Dataset/test.csv', limit=10)
```

---

## âš™ï¸ Configuration

Key parameters in `advanced_rag/config.py`:

| Setting | Default | Description |
|---------|---------|-------------|
| `embedding.model_name` | `BAAI/bge-large-en-v1.5` | Embedding model |
| `chunking.parent_chunk_size` | 2048 | Parent chunk size in tokens |
| `chunking.child_chunk_size` | 256 | Child chunk size in tokens |
| `retrieval.semantic_weight` | 0.6 | Weight for semantic search |
| `retrieval.bm25_weight` | 0.4 | Weight for BM25 search |
| `llm.model_name` | `llama-3.1-8b-instant` | LLM for verification |
| `llm.self_consistency_samples` | 1 | Self-consistency iterations |

---

## ğŸ“Š Expected Performance

| Metric | Target | Method |
|--------|--------|--------|
| Hallucination Rate | <1% | Citation grounding + CoVe |
| Context Precision | >85% | Hybrid retrieval + reranking |
| Faithfulness | >95% | Self-consistency decoding |
| Latency (GPU) | ~5s/claim | Adaptive routing |

---

## ğŸ”‘ API Keys

Get your free Groq API key at [console.groq.com](https://console.groq.com)

---

## ğŸ“ License

MIT License

---

## ğŸ™ Acknowledgments

- [LlamaIndex](https://www.llamaindex.ai/) for RAG patterns
- [RAGAS](https://github.com/explodinggradients/ragas) for evaluation metrics
- [Instructor](https://github.com/jxnl/instructor) for structured LLM outputs